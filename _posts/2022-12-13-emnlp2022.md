---
layout: post
---

We presented [several papers at EMNLP 2022](https://twitter.com/MaiNLPlab/status/1600795488605073409) in Abu Dhabi, amonst which here are some selected highlight on on data-centric AI problems. 

This includes a paper on discussing the problem of *human label variation* in AI, which arises when human annotators assign different valid labels to the same item. Yet, most AI systems today are trained on the assumption that there exists a single ``ground truth'', or, a single valid interpretation per item:

- Barbara Plank. [*The "Problem” of Human Label Variation: On Ground Truth in Data, Modeling and Evaluation.*](https://aclanthology.org/2022.emnlp-main.731/) In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 2022.

Calibration is a popular framework to evaluate whether a neural networks knows when it does not know - i.e., its predictive probabilities are a good indication of how likely a prediction is to be correct. Correctness is commonly estimated against the human majority class (a single ground truth). What does this mean in light of human label variation? Read up here:

- Joris Baan, Wilker Aziz, Barbara Plank and Raquel Fernández. [*Stop Measuring Calibration When Humans Disagree*.](https://aclanthology.org/2022.emnlp-main.124/) In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 2022. 



[Img credits to Max-Müller Eberstein](https://twitter.com/mxmeij/status/1601832608073388032)
