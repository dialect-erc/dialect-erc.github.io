---
---

@inproceedings{blaschke-etal-2023-survey,
    teaser="This paper provides an overview of more than 80 corpora to support NLP research in resource-poor and non-standardized languages of the Germanic language family.",
    image = "GerLowResourceVarieties.png",
    title = "A Survey of Corpora for {G}ermanic Low-Resource Languages and Dialects",
    author = "Blaschke, Verena  and
      Schuetze, Hinrich  and
      Plank, Barbara",
    editor = {Alum{\"a}e, Tanel  and
      Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.41",
    pages = "392--414",
    abstract = "Despite much progress in recent years, the vast majority of work in natural language processing (NLP) is on standard languages with many speakers. In this work, we instead focus on low-resource languages and in particular non-standardized low-resource languages. Even within branches of major language families, often considered well-researched, little is known about the extent and type of available resources and what the major NLP challenges are for these language varieties. The first step to address this situation is a systematic survey of available corpora (most importantly, annotated corpora, which are particularly valuable for NLP research). Focusing on Germanic low-resource language varieties, we provide such a survey in this paper. Except for geolocation (origin of speaker or document), we find that manually annotated linguistic resources are sparse and, if they exist, mostly cover morphosyntax. Despite this lack of resources, we observe that interest in this area is increasing: there is active development and a growing research community. To facilitate research, we make our overview of over 80 corpora publicly available.",
}


@inproceedings{wang-etal-2024-answer-c,
    teaser="This paper investigates to what extent the first token probabilities of large language models match their final answers to multiple-choice questions.",
    url = "https://aclanthology.org/2024.findings-acl.441/",
    image = "first-token-prob.png",
    title = "``My Answer is {C}'': First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models",
    author = {Wang, Xinpeng  and
      Ma, Bolei  and
      Hu, Chengzhi  and
      Weber-Genzel, Leon  and
      R{\"o}ttger, Paul  and
      Kreuter, Frauke  and
      Hovy, Dirk  and
      Plank, Barbara},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    doi = "10.18653/v1/2024.findings-acl.441",
    pages = "7407--7416",
    abstract = "The open-ended nature of language generation makes the evaluation of autoregressive large language models (LLMs) challenging. One common evaluation approach uses multiple-choice questions to limit the response space. The model is then evaluated by ranking the candidate answers by the log probability of the first token prediction. However, first-tokens may not consistently reflect the final response output, due to model{'}s diverse response styles such as starting with ``Sure'' or refusing to answer. Consequently, first-token evaluation is not indicative of model behaviour when interacting with users. But by how much? We evaluate how aligned first-token evaluation is with the text output along several dimensions, namely final option choice, refusal rate, choice distribution and robustness under prompt perturbation. Our results show that the two approaches are severely misaligned \textit{on all dimensions}, reaching mismatch rates over 60{\%}. Models heavily fine-tuned on conversational or safety data are especially impacted. Crucially, models remain misaligned even when we increasingly constrain prompts, i.e., force them to start with an option letter or example template. Our findings i) underscore the importance of inspecting the text output as well and ii) caution against relying solely on first-token evaluation."
}

@inproceedings{blaschke25_interspeech,
    image = "bayern_regierungsbezirke_dialekte.png",
    url = "https://www.isca-archive.org/interspeech_2025/blaschke25_interspeech.html",
    teaser ="Although Germany has a diverse landscape of dialects, they are underrepresented in current automatic speech recognition (ASR) research. To enable studies of how robust models are towards dialectal variation, we present Betthupferl, a new benchmark for transcription into dialect and standard for three dialect groups in Southeast Germany.",
    title = {{A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation}},
    author = {Verena Blaschke and Miriam Winkler and Constantin FÃ¶rster and Gabriele Wenger-Glemser and Barbara Plank},
    year = {2025},
    month = aug,
    booktitle = {{Interspeech 2025}},
    pages = {913--917},
    doi = {10.21437/Interspeech.2025-318},
    issn = {2958-1796},
}
